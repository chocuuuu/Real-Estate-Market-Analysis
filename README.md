# Real-Estate Market Analysis

This project is a **web scraping solution** designed to extract and analyze real estate data from [Zillow.com](https://www.zillow.com/).  
The core of this project is an automated workflow leveraging the **Apify platform** to scrape property listings and save them into a CSV file, providing a clean dataset for future analysis.

---

## ğŸ“Œ Project Status
Currently in the **data gathering phase**.  
- âœ… Collecting a comprehensive dataset of Zillow properties.  
- â³ Data analysis and visualization are **work in progress**.  

---

## ğŸ—‚ï¸ Scraped Data Fields
The scraper collects a wide range of attributes for each property listing.  
Key fields include:

- **`zpid`** â†’ Unique property ID on Zillow  
- **`address`** â†’ Full property address  
- **`location`** â†’ Geographic location / region  
- **`price`** â†’ Current listing price  
- **`bedrooms`** â†’ Number of bedrooms  
- **`bathrooms`** â†’ Number of bathrooms  
- **`livingArea`** â†’ Square footage of the property  
- **`lotSizeWithUnit`** â†’ Lot size (with units)  
- **`yearBuilt`** â†’ Year the property was built  
- **`propertyType`** â†’ Type of property (house, condo, etc.)  
- **`daysOnZillow`** â†’ How long the listing has been active  
- **`listingDateTimeOnZillow`** â†’ Date/time first listed  
- **`estimates`** â†’ Zillowâ€™s price estimate data (Zestimate, rent estimate, etc.)  
- **`openHouseShowingList`** â†’ Upcoming open house events (if available)  

These fields provide the **core dataset** for pricing trends, housing supply analysis, and property comparisons.

---

## âœ¨ Key Features
- **Automated Web Scraping** â†’ Python script initiates & manages an Apify actor.  
- **Real-time Data** â†’ Scrapes up-to-date Zillow listings with configurable search parameters.  
- **Data Export** â†’ Saves to `zillow_properties.csv`.  
- **GitHub Actions Integration** â†’ Workflow automates scraping & data export on push to `main`.  
- **Secure Credential Management** â†’ API tokens & IDs stored safely in **GitHub Secrets**.  

---

## âš™ï¸ How It Works
The automation is handled by a **GitHub Actions workflow**:  
`.github/workflows/apify_workflow.yml`

1. Push to `main` or manually trigger the workflow.  
2. GitHub injects **credentials** from repository secrets into the environment.  
3. `main.py` script runs, connecting to Apify API.  
4. Script launches the **Zillow scraper** and fetches results.  
5. Data is saved to `zillow_properties.csv`.  
6. Workflowâ€™s **upload-artifact step** saves the CSV as a downloadable artifact.  

---

## ğŸš€ Setup for New Users
To use this project in your own GitHub account:

1. **Fork this repository.**  
2. Go to **Settings > Secrets and variables > Actions** in your fork.  
3. Add the following **repository secrets** (from your Apify account):
   - `APIFY_TOKEN`
   - `ACTOR_RUN_ID`
   - `KEY_VALUE_STORE_ID`
   - `DATASET_ID`
4. Push to your `main` branch.  
   - The scraping workflow will **automatically run**.  
   - Download the generated `zillow-data.zip` artifact from the **Actions tab**.  

---

## ğŸ“Š Next Steps
- ğŸ” Perform exploratory data analysis (EDA).  
- ğŸ“ˆ Build dashboards & visualizations.  
- ğŸ§  Apply machine learning for **price predictions & market trends**.  

---
